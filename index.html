<!doctype html>
<html>
    <head>
        <meta charset="utf-8"/>

        <title>Learning where to look: a foveated visuomotor control model - CNS*2019</title>

        <meta name="description" content="Learning where to look: a foveated visuomotor control model">
        <meta name="author" content="Emmanuel Daucé, Pierre Albigès & Laurent Perrinet">

        <meta name="apple-mobile-web-app-capable" content="yes" >
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <!-- General and theme style sheets -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/css/reveal.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/css/theme/simple.css" id="theme">

    	<!-- Theme used for syntax highlighting of code -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/lib/css/zenburn.css">

        <!-- Printing and PDF exports -->
         <script>
                 var link = document.createElement( 'link' );
                 link.rel = 'stylesheet';
                 link.type = 'text/css';
                 link.href = window.location.search.match( /print-pdf/gi ) ? 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/css/print/pdf.css' : 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/css/print/paper.css';
                 document.getElementsByTagName( 'head' )[0].appendChild( link );
         </script>

         <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
         <!--[if lt IE 9]>
         <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/lib/js/html5shiv.js"></script>
         <![endif]-->
    
        <!-- Get Font-awesome from cdn -->
        <!-- <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.css"> -->
    </head>

<body>
    <div class="reveal">
        <div class="slides">
        <section><section>
<h2 class="title">Learning where to look: a foveated visuomotor control model</h2>
<h3><a href="https://laurentperrinet.github.io/talk/2019-07-15-cns/">Emmanuel Daucé, Pierre Albigès & Laurent Perrinet</a></h3>
<img class="plain" data-src="http://laurentperrinet.github.io/slides.py/figures/troislogos.png"  height="200.0px" />
<h4><a href="https://www.cnsorg.org/cns-2019">CNS*2019</a>, 15/7/2019 </h4>


<small>
<h5>Acknowledgements:</h5>
<ul>
    <li>Rick Adams and Karl Friston @ UCL - Wellcome Trust Centre for Neuroimaging</li>
    <li>Jean-Bernard Damasse and Laurent Madelain - ANR REM</li>
    <li>Frédéric Chavane - INT</li>
</ul>
<BR>
<img class="plain" data-src="https://laurentperrinet.github.io/authors/rick-a.-adams/avatar.jpg"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/karl-friston/avatar.jpg"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/jean-bernard-damasse/avatar.jpg"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/laurent-madelain/avatar.png"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/anna-montagnini/avatar.jpg"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/frédéric-chavane/avatar.png"  height="80px" /><a href="https://github.com/laurentperrinet/slides.py"><img class="plain" data-src="https://www.python.org/static/community_logos/python-powered-h-140x182.png"  height="80px" /></a>
<BR>
    This work was supported by the <a href="https://laurentperrinet.github.io/project/pace-itn/">PACE-ITN Project</a>.
</small>



                <aside class="notes">
                 <ul>
<li>(AUTHOR) Hello, I am Laurent Perrinet from the Institute of Neurosciences of
la Timone in Marseille, a joint unit from the CNRS and the AMU</li>
</ul>
                </aside>
                
</section>
        <section><h3>Outline</h3>
<ol>

                    <h3>
                    <li>
                    <p class="fragment highlight-red">
                    Motivation
                    </p>
                    </li>
                    </h3>
                    
                    <h3>
                    <li>
                    Methods
                    </li>
                    </h3>
                    
                    <h3>
                    <li>
                    Results
                    </li>
                    </h3>
                    
                    <h3>
                    <li>
                    Conclusion
                    </li>
                    </h3>
                    
                     </ol>
                    
                <aside class="notes">
                 
                </aside>
                
</section>
        <section><h3>Computer vision</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=825 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/CNS-general-I.svg"  height="825px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        
                <aside class="notes">
                 <ul>
<li>(OBJECTIVE)</li>
</ul>
<p>Past 5-10 years have seen a huge development of machine learning/deep learning based image processing, indeed artificial vision has been revolutioned by
the incredible capability of convolution-based deep networks to capture the semantic content of images/photographs. Their success relies on a reduction of parameter complexity
through weight sharing convolutional neural networks applied over the full image. In order to increase the recognition capability, there has been an inflation in the number of layers needed
to process the pixel information. Finally, the processing of large images can be done at a cost that scales quadratically with the image resolution. All regions, even the “boring” ones are
systematically scanned and processed in parallel fashion at high computational cost.</p>
                </aside>
                
</section>
        <section><h3>Human vision</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=825 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/CNS-general-II.svg"  height="825px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        
                <aside class="notes">
                 <ul>
<li>(OBJECTIVE)</li>
</ul>
<p>When human vision is considered, the things work quite differently.
The human vision is <strong>dynamic</strong>.
Human (and animal) vision rely on a non isotropic sensor (the retina) that has a very high resolution at the center of fixation and a very poor
resolution at the periphery. Most importantly, the human vision is dynamic. The scanning of a full visual scene is not done in parallel but sequentially, and only scene-relevant regions of interest are scanned through saccades. This implies a <strong>decision process</strong> at each step that decides <strong>where to look next</strong>.</p>
                </aside>
                
</section>
        <section><h3>Statistical Viewpoint</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=825 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/CNS - Modelling - I.svg"  height="825px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        
                <aside class="notes">
                 <p>This kind of reasoning can be captured by a statistical framework called a POMDP (partially observed Markov Decision Process) where the cause of a visual scene is couple made of 
a viewpoint and scene elements. Changing the viewpoint will conduct to a different scene rendering. Knowing the current view, you need to choose the next viewpoint that will help you to 
disambiguate the scene. </p>
<p>In a classic inference framework, a (generative) model tells how typically looks the visual field knowing the scene elements and a certain viewpoint . Using bayes rule, you may then infer the scene elements from the 
current view point (model inversion).  </p>
<p>The more viewpoints you have, the more certain you are about the content of the scene.</p>
                </aside>
                
</section>
        <section><h3>Attention vs. Scene Understanding</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=825 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/CNS - Modelling - II.svg"  height="825px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        
                <aside class="notes">
                 <p>Bottom up :</p>
<p>Uses <strong>local</strong> image statistics to estimate which part of the image departs the most from the baseline statistiscs </p>
<ul>
<li>Laurent Itti and Christof Koch. <strong>A saliency-based search mechanism
    for overt and covert shifts of visual attention</strong>. In: Vision
    Research 40.10-12 (2000), pp. 1489--1506.</li>
<li>M. Kümmerer, L. Theis, and M. Bethge <strong>Deep Gaze I: Boosting
    Saliency Prediction with Feature Maps Trained on ImageNet</strong> ICLR
    Workshop, 2015</li>
</ul>
<p>Top down : (sequential decision)</p>
<p>In an active inference setup means using a generative model to quantify the benefit of doing a certain action (changing viewpoint) to reduce the <strong>posterior entropy</strong> given an history of past actions (viewpoints)</p>
<ul>
<li>J Najemnik and Wilson S. Geisler. <strong>Optimal eye movement
        strategies in visual search</strong>. In: Nature reviews. Neuroscience
        434 (2005)</li>
<li>Nicholas J Butko and Javier R Movellan. <strong>Infomax control of eye
        movements</strong>. In: Autonomous Mental Development, IEEE
        Transactions on 2.2 (2010)</li>
<li>Fu, J., Zheng, H., &amp; Mei, T. (2017). Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4438-4446).</li>
</ul>
                </aside>
                
</section>
        
</section>
<section><section><h3>Outline</h3>
<ol>

                    <h3>
                    <li>
                    Motivation
                    </li>
                    </h3>
                    
                    <h3>
                    <li>
                    <p class="fragment highlight-red">
                    Methods
                    </p>
                    </li>
                    </h3>
                    
                    <h3>
                    <li>
                    Results
                    </li>
                    </h3>
                    
                    <h3>
                    <li>
                    Conclusion
                    </li>
                    </h3>
                    
                     </ol>
                    
                <aside class="notes">
                 <p>Indeed, we will use the separation of the 2 problemes (where and what) as they are confronted to nuisances of different kinds</p>
                </aside>
                
</section>
        <section><h3>Active Vision</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=825 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/CNS-what-where-diagram.svg"  height="825px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        
                <aside class="notes">
                 <p>So what we propose here is to go a little further in a biomimetic implementation of an artificial vision system. 
(Why : biomimetic systems are the result of a continual optimization throughout ages of evolution: they optimize signal processing under strong material and energy constraints, for specific surfival purposes.)</p>
<p>Objective : build an effective artificial foveal vision 
We concentrate her on the foveal vision case
What is specific with foveal vision? 
Foveal vision is a trick that was selected by natural selection : a compromise between resource saving and accuracy (budgeted vision)<br />
The fovea that concentrates most of the photoreceptors, represents less than 2% of the total visual field
In a foveal vision setting, the current view may allow you to tell there is an object of interest in your peripheral vision (for instance a face),that you can not identify, and you need to make a saccade to 
identify the person.</p>
<p>So in order to analyze a complex visual scene, there are two types of processing that need to be done. On the one side, you need  to process in detail what is at the center of fixation, that is the region of interest currently processed. On the other side, you also need to analyze the surrounding part, even if the resolution is low, in order to choose what is the next position of fixation. This basically means making a choice of “what’s interesting next”. You do not necessarily need to know what it is, but you need to that it’s interesting enough, and of course you need to know what action to take to move the center of fixation at the right position.</p>
<p>go further in the predictive coding framework</p>
<p>motivation : can we make a network that detects where <em>before</em> actually knowing what?</p>
<p>what is the most ecological (best compression of information) to achieve that in the form of log-polar maps for instance?</p>
                </aside>
                
</section>
        <section><h3>Methods</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=825 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/fig_intro.jpg"  height="825px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        
                <aside class="notes">
                 <p>protocol</p>
<p>We consider here a restricted setup is the one that could be used in a psychophysic experiment for a visual search task. 
This setup allows to control the difficulty of the task and test our foveal vision in different conditions.
we control :
background noise frequency (crowding), 
target contrast, 
target eccentricity </p>
<p>TODO-LAurent = génére les frames pour un "film"</p>
                </aside>
                
</section>
        <section><h3>Methods</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=825 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/CNS-what-diagram.svg"  height="825px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        
                <aside class="notes">
                 
                </aside>
                
</section>
        <section><h3>Methods</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=825 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/CNS-where-diagram.svg"  height="825px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        
                <aside class="notes">
                 
                </aside>
                
</section>
        
</section>
<section><section><h3>Outline</h3>
<ol>

                    <h3>
                    <li>
                    Motivation
                    </li>
                    </h3>
                    
                    <h3>
                    <li>
                    Methods
                    </li>
                    </h3>
                    
                    <h3>
                    <li>
                    <p class="fragment highlight-red">
                    Results
                    </p>
                    </li>
                    </h3>
                    
                    <h3>
                    <li>
                    Conclusion
                    </li>
                    </h3>
                    
                     </ol>
                    
                <aside class="notes">
                 <p>Indeed, t...</p>
                </aside>
                
</section>
        <section><h3>Results- correct</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
                <tr style="vertical-align:middle" bgcolor="white"  height="412px">
                    <td width="100%" style="text-align:center; vertical-align:middle" bgcolor="white" >
                    <p>
                        <img class="plain" data-src="figures/fig_result.jpg"  height="412px"  />
                    </p>
                    </td>
                </tr>
                
                <tr style="vertical-align:middle" bgcolor="white"  height="412px">
                    <td width="100%" style="text-align:center; vertical-align:middle" bgcolor="white" >
                    <p>
                        <img class="plain" data-src="figures/fig_result.jpg"  height="412px"  />
                    </p>
                    </td>
                </tr>
                
        </table>
        </div>
        
                <aside class="notes">
                 <pre><code>TODO Manu : générer images correctes avec leur saccades + incorrectes (fake)
</code></pre>
                </aside>
                
</section>
        <section><h3>Results- error</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
                <tr style="vertical-align:middle" bgcolor="white"  height="412px">
                    <td width="100%" style="text-align:center; vertical-align:middle" bgcolor="white" >
                    <p>
                        <img class="plain" data-src="figures/fig_result.jpg"  height="412px"  />
                    </p>
                    </td>
                </tr>
                
                <tr style="vertical-align:middle" bgcolor="white"  height="412px">
                    <td width="100%" style="text-align:center; vertical-align:middle" bgcolor="white" >
                    <p>
                        <img class="plain" data-src="figures/fig_result.jpg"  height="412px"  />
                    </p>
                    </td>
                </tr>
                
        </table>
        </div>
        
                <aside class="notes">
                 <pre><code>TODO Manu : générer images correctes avec leur saccades + incorrectes (fake)
</code></pre>
                </aside>
                
</section>
        <section><h3>Results</h3>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=825.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=825 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/fig_params.jpg"  height="825px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        
                <aside class="notes">
                 <p>TODO Manu : insérer résultats avec différents contrastes</p>
                </aside>
                
</section>
        
</section>
<section>
<section data-markdown>
<script type="text/template">
        
# Bayesian Online Changepoint Detector

* an implementation of
[Adams &amp; MacKay 2007 "Bayesian Online Changepoint Detection"](http://arxiv.org/abs/0710.3742)
in Python.

````
@TECHREPORT{ adams-mackay-2007,
AUTHOR = "Ryan Prescott Adams and David J.C. MacKay",
TITLE  = "Bayesian Online Changepoint Detection",
INSTITUTION = "University of Cambridge",
ADDRESS = "Cambridge, UK",
YEAR = "2007",
NOTE = "arXiv:0710.3742v1 [stat.ML]",
URL = "http://arxiv.org/abs/0710.3742"
}
````

* adapted from https://github.com/JackKelly/bayesianchangepoint by Jack Kelly (2013) for a binomial input.

* This code is based on the  [MATLAB implementation](http://www.inference.phy.cam.ac.uk/rpa23/changepoint.php) provided by Ryan Adam. Was available at http://hips.seas.harvard.edu/content/bayesian-online-changepoint-detection

* full code @ https://github.com/laurentperrinet/bayesianchangepoint


</script>
            
                <aside class="notes">
                 <p>TODO Manu update with perspectives</p>
                </aside>
                
</section>
        <section>
<h2 class="title">Learning where to look: a foveated visuomotor control model</h2>
<h3><a href="https://laurentperrinet.github.io/talk/2019-07-15-cns/">Emmanuel Daucé, Pierre Albigès & Laurent Perrinet</a></h3>
<img class="plain" data-src="http://laurentperrinet.github.io/slides.py/figures/troislogos.png"  height="200.0px" />
<h4><a href="https://www.cnsorg.org/cns-2019">CNS*2019</a>, 15/7/2019 </h4>


<small>
<h5>Acknowledgements:</h5>
<ul>
    <li>Rick Adams and Karl Friston @ UCL - Wellcome Trust Centre for Neuroimaging</li>
    <li>Jean-Bernard Damasse and Laurent Madelain - ANR REM</li>
    <li>Frédéric Chavane - INT</li>
</ul>
<BR>
<img class="plain" data-src="https://laurentperrinet.github.io/authors/rick-a.-adams/avatar.jpg"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/karl-friston/avatar.jpg"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/jean-bernard-damasse/avatar.jpg"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/laurent-madelain/avatar.png"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/anna-montagnini/avatar.jpg"  height="80px" /><img class="plain" data-src="https://laurentperrinet.github.io/authors/frédéric-chavane/avatar.png"  height="80px" /><a href="https://github.com/laurentperrinet/slides.py"><img class="plain" data-src="https://www.python.org/static/community_logos/python-powered-h-140x182.png"  height="80px" /></a>
<BR>
    This work was supported by the <a href="https://laurentperrinet.github.io/project/pace-itn/">PACE-ITN Project</a>.
</small>



                <aside class="notes">
                 <p>perspectives:
- XXX
- XXX</p>
<ul>
<li>Thanks for your attention!</li>
</ul>
                </aside>
                
</section>
        <section>
            <div align="center">
            <table border=0px VALIGN="center" bgcolor=white height=700.0>
            
            <tr padding=0px style="vertical-align:middle" bgcolor=white>
            
                <td height=700 width="1600" padding-top=0px padding-bottom=0px style="text-align:center; vertical-align:middle" bgcolor="white" >
                <p>
                    <img class="plain" data-src="figures/qr.png"  height="700px"   />
                </p>
                </td>
                
            </tr>
            
        </table>
        </div>
        <BR><a href="https://laurentperrinet.github.io/talk/2019-07-15-cns"> https://laurentperrinet.github.io/talk/2019-07-15-cns </a>
                <aside class="notes">
                 <p>All the material is available online - please flash this code this leads to a page with links to further references and code - TODO : use ArXiV instead </p>
                </aside>
                
</section>
        
</section>

        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/lib/js/head.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/js/reveal.js"></script>

        
	<script>

            // Full list of configuration options available at:
            // https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({

        
                // The "normal" size of the presentation, aspect ratio will be preserved
                // when the presentation is scaled to fit different resolutions. Can be
                // specified using percentage units.
                width: 1600,
                height: 1000,

                // Factor of the display size that should remain empty around the content
                margin: 0.1618,
        
                // Display a presentation progress bar
                progress: true,
                slideNumber: 'c/t',

                // Push each slide change to the browser history
                //history: false,

                // Vertical centering of slides
                center: false,

                // Enables touch navigation on devices with touch input
                touch: true,

                // Bounds for smallest/largest possible scale to apply to content
                minScale: 0.2,
                maxScale: 2.5,

                // Display controls in the bottom right corner
                controls: false,

                // Enable keyboard shortcuts for navigation
                keyboard: true,

                // Enable the slide overview mode
                overview: true,

                // Loop the presentation
                //loop: false,

                // Change the presentation direction to be RTL
                //rtl: false,

                // Number of milliseconds between automatically proceeding to the
                // next slide, disabled when set to 0, this value can be overwritten
                // by using a data-autoslide attribute on your slides
                //autoSlide: 0,

                // Enable slide navigation via mouse wheel
                //mouseWheel: false,

                // Parallax background image
                //parallaxBackgroundImage: '/Users/laurentperrinet/cloud_nas/2015_RTC/2014-04-17_HDR/figures/p4100011.jpg', // e.g. "https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg"

                // Parallax background size
                //parallaxBackgroundSize: '3200px 2000px', // CSS syntax, e.g. "2100px 900px" - currently only pixels are supported (don't use % or auto)

                // This slide transition gives best results:
                transition: 'fade', // default/cube/page/concave/zoom/linear/fade/none

                // Transition speed
                transitionSpeed: 'slow', // default/fast/slow

                // Transition style for full page backgrounds
                backgroundTransition: 'none', // default/linear/none

			    // Turns fragments on and off globally
                fragments: true,

                // Theme
                theme: 'simple', // available themes are in /css/theme

        
                math: {
            		mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
                    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                },
            	chalkboard: {
            		// optionally load pre-recorded chalkboard drawing from file
            		src: "chalkboard.json",
            	},
                // Optional reveal.js plugins
                dependencies: [
                        { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                        { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                        { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                        { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                        { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/plugin/chalkboard/chalkboard.js' },
                        { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/plugin/zoom-js/zoom.js', async: true },
                        { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/plugin/notes/notes.js', async: true },
                        { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/plugin/math/math.js', async: true },
                        { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.7.0/plugin/mathsvg/math.js', async: true },
        
                ],
	keyboard: {
	    67: function() {{ RevealChalkboard.toggleNotesCanvas() }},	// toggle notes canvas when 'c' is pressed
	    66: function() {{ RevealChalkboard.toggleChalkboard() }},	// toggle chalkboard when 'b' is pressed
	    46: function() {{ RevealChalkboard.clear() }},	// clear chalkboard when 'DEL' is pressed
	     8: function() {{ RevealChalkboard.reset() }},	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
	    68: function() {{ RevealChalkboard.download() }},	// downlad recorded chalkboard drawing when 'd' is pressed
	},
        });
        </script>



    </body>
</html>
        